{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mingu Kim- 5/22/17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of Structured Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$r_t \\sim N(\\theta(s_t, a_t), \\tau^{-1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_r(theta_val):\n",
    "    return np.random.normal(theta_val, tau ** (-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta(s,a) \\sim N(0, \\lambda_0^{-1}(s,a))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def theta(s, a):\n",
    "    return np.random.normal(0, lambda_1(s,a) ** (-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we assume precisions are the same for all states, actions **\n",
    "$$\\lambda_1(s,a) = \\lambda_0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lambda_1(s,a):\n",
    "    return lambda0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lambda_t(s_t,a_t) = \\lambda_{t-1}(s_t,a_t) + \\tau$$\n",
    "$$\\lambda_t(s_t, a_t) = \\lambda_1(s,a) + (t-1) \\tau$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lambda_t(t, s, a):\n",
    "    return 0.1 * (t-1) * tau + lambda_1(s, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\eta_t = \\frac{\\tau}{\\tau + \\lambda_t(s_t,a_t)}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eta(t, s, a):\n",
    "    return tau / (tau + lambda_t(t, s, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\theta}_t(s_t,a_t) =\\hat{\\theta}_{t-1} (s_t,a_t) + \\alpha_s \\eta_t [r_{t-1} - \\hat{\\theta}_{t-1}(s_t,a_t)] $$\n",
    "We also know that $$\\hat{\\theta}_1(s_t,a_t) = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** I used a dictionary to implement my posterior means, so that we can flexibly add (s,a) pairs as needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def theta_t(thetas, r, s, a, t):\n",
    "    updated_thetas = thetas.copy()\n",
    "    if (s,a) not in thetas:\n",
    "        updated_thetas[(s, a)] = eta(1, s, a) * r\n",
    "    else:\n",
    "        updated_thetas[(s, a)] \\\n",
    "        = updated_thetas[(s, a)] + eta(t, s, a) * (r - updated_thetas[(s, a)])\n",
    "    return updated_thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(r_t \\mid s_t, a_t) = N(r_t; \\hat{\\theta}_t (s_t, a_t), \\lambda_t(s_t, a_t)^{-1} + \\tau^{-1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(r, t, s, a, thetas):\n",
    "    return norm(thetas.get((s, a), 0), tau ** (-1) + lambda_t(t, s, a) ** (-1)).pdf(r) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "  P(s_t = k \\mid \\textbf{s}_{1:t-1}) =\n",
    "  \\begin{cases}\n",
    "   \\frac{C_k + \\omega \\mathbb{I}[s_t = s_{t-1}]}{t+\\alpha - 1} & \\text{if $k \\leq K$} \\\\\n",
    "   \\frac{\\alpha}{t+\\alpha-1} & \\text{if $k = K_1$} \n",
    "  \\end{cases}\n",
    "$$\n",
    "where $\\mathbb{I}$[·] = 1 when its argument is true (0 otherwise), $C_k$ is the number of previous trials state $k$\n",
    "was sampled, and $K$ is the total number of distinct previously sampled states. The concentration\n",
    "parameter $\\alpha > 0$ controls the propensity for sampling new states; the expected number of distinct\n",
    "states after $t$ trials is $\\alpha \\ln t$. When $\\alpha = 0$, only a single state will be sampled for all trials, and when\n",
    "α approaches infinity, each trial will be associated with a distinct state. The parameter $\\omega$ specifies\n",
    "the “stickiness” or autocorrelation of states: when stickiness is high, consecutive trials will tend to\n",
    "sample the same state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Takes k, $\\omega$, $\\alpha$, and visited states\n",
    "def prior_prob(k,w, alpha, s_list):\n",
    "    K = len(set(s_list))\n",
    "    if k <= K - 1:\n",
    "        return (s_list.count(k) + w * int(k == s_list[-1])) / (len(s_list) + alpha)\n",
    "    elif k == K:\n",
    "        return alpha / (len(s_list) + alpha)\n",
    "    else:\n",
    "        return \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gives normalized probabilities of visiting each state\n",
    "def prior_norm(w, alpha, s_list):\n",
    "    K = len(set(s_list))\n",
    "    probs = np.zeros(K + 1)\n",
    "    for k in range(K+1):\n",
    "        probs[k] = prior_prob(k,w,alpha, s_list)\n",
    "    return np.array([float(i)/sum(probs) for i in probs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\DeclareMathOperator*{\\argmax}{argmax}$$\n",
    "$$P(s_t \\mid \\textbf{a}_{1:t-1}, \\textbf{r}_{1:t-1}) \\approx P(s_t \\mid \\textbf{a}_{1:t-1}, \\textbf{r}_{1:t-1}, \\hat{\\textbf{s}}_{1:t-1}), $$\n",
    "\n",
    "where $\\hat{s}_t$ is defined recursively as follows:\n",
    "\n",
    "$$\\hat{s}_t = \\argmax_{s_t} P(r_t \\mid s_t, \\textbf{a}_{1:t}, \\textbf{r}_{1:t-1}, \\hat{\\textbf{s}}_{1:t-1}) P(s_t \\mid \\hat{\\textbf{s}}_{1:t-1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def s_hat(s_list, t, r, a, w, alpha, thetas):\n",
    "    priors = prior_norm(w, alpha, s_list)\n",
    "    #print(priors)\n",
    "    s_hats = np.array([likelihood(r, t, s, a, thetas) * priors[s] for s in range(len(priors))])\n",
    "    return np.argmax(s_hats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure if I wrote it right, but the second one is true to recursive definition, while first one is a shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prior_map_est(s_list, reward_list, action_list, w, alpha, thetas):\n",
    "    if len(reward_list) == 0:\n",
    "        return [s_list, [1]]\n",
    "    s_list.append(s_hat(s_list, i, reward_list[-1], action_list[-1], w, alpha, thetas))\n",
    "    return [s_list, prior_norm(w, alpha, s_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prior_map_est(reward_list, action_list, w, alpha, thetas):\n",
    "    s_list = [0] * (len(reward_list) + 1)\n",
    "    for i in range(len(reward_list)):\n",
    "        s_list[i+1] = s_hat(s_list[:i+1], i, reward_list[i], action_list[i], w, alpha, thetas)\n",
    "    return prior_norm(w, alpha, s_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$b_t(s) \\propto P(r_t \\mid s_t = s, a_t) P(s_t = s \\mid \\textbf{a}_{1:t-1}, \\textbf{r}_{1:t-1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def posterior(prior, state, last_action, reward_list, w, alpha, thetas):   \n",
    "    if state > len(prior):\n",
    "        return 0\n",
    "    return likelihood(reward_list[-1], len(reward_list), state, last_action, thetas) * prior[state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def posterior_norm(prior, last_action, reward_list, w, alpha, thetas):   \n",
    "    probs = np.zeros(len(prior))\n",
    "    for k in range(len(prior)):\n",
    "        probs[k] = posterior(prior, k, last_action, reward_list, w, alpha, thetas)\n",
    "    return np.array([float(i)/sum(probs) for i in probs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mu_t(a) = \\sum_s b_t(s) \\hat{\\theta}_t(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mu_t(prior,action, reward_list, w, alpha, thetas):\n",
    "    exp_reward = 0\n",
    "    for state in range(len(prior)):\n",
    "        exp_reward += posterior(prior, state, action, reward_list, w, alpha, thetas) \\\n",
    "        * thetas.get((state, action), 0)\n",
    "    return exp_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi_t(s) = e^{\\beta \\mu_{t-1} (a)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_action(prior, actions, action_list, reward_list, w, alpha, thetas, beta):\n",
    "    if len(action_list) == 0:\n",
    "        return np.random.choice(len(actions))\n",
    "    action_vals = []\n",
    "    for action in actions:\n",
    "        action_vals.append(np.exp(beta * mu_t(prior, action, reward_list, w, alpha, thetas)))\n",
    "    actions_normed = np.array(action_vals) / sum(action_vals)\n",
    "    return np.random.choice(len(actions), 1, p=actions_normed)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q_t(a) = \\mu_{t-1} (a) + \\beta^{-1} \\sigma_{t-1}(a)$$\n",
    "\n",
    "$$\\sigma_t(a) = \\sqrt{\\sum_s b_t(s) [(\\hat{\\theta}_t(s,a) - \\mu_t(s,a))^2 + 1/\\lambda_t(s,a)]}$$\n",
    "\n",
    "$$\\pi_t(a) = (1-\\epsilon) \\mathbb{I}[a=a^{*}] + \\frac{\\epsilon}{A} (1-\\mathbb{I}[a=a^{*}]))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def update_thetas(thetas, r, a, prior):\n",
    "#     updated_thetas = thetas.copy()\n",
    "#     s = np.random.choice(len(prior), p = prior)\n",
    "#     try:\n",
    "#         updated_thetas[(s, a)] \\\n",
    "#         = [updated_thetas[(s, a)][0] + eta(updated_thetas[(s, a)][1], s, a) * (r - updated_thetas[(s, a)][0]), updated_thetas[(s, a)][1] + 1]\n",
    "#     except KeyError:\n",
    "#         updated_thetas[(s, a)] = [eta(1, s, a) * r, 2]\n",
    "#     return updated_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_thetas(thetas, r, a, posterior, t):\n",
    "    updated_thetas = thetas.copy()\n",
    "    for s in range(len(posterior)):\n",
    "        try:\n",
    "            updated_thetas[(s, a)] \\\n",
    "            = updated_thetas[(s, a)] + posterior[s] * eta(t, s, a) * (r - updated_thetas[(s, a)])\n",
    "        except KeyError:\n",
    "            updated_thetas[(s, a)] = posterior[s] * eta(1, s, a) * r\n",
    "    return updated_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def UCB_action(prior, actions, action_list, reward_list, w, alpha, thetas, beta, epsilon):\n",
    "    if len(action_list) == 0:\n",
    "        return np.random.choice(len(actions))\n",
    "    action_vals = []\n",
    "    Q = []\n",
    "    for action in actions:\n",
    "        sigma = 0\n",
    "        for s in range(len(prior)):\n",
    "            sigma+= posterior(prior, s, action_list[-1], reward_list, w, alpha, thetas) \\\n",
    "            * ((thetas.get((s, action), 0) - mu_t(prior, action, reward_list, w, alpha, thetas))**2\\\n",
    "            + 1 / lambda_t(len(reward_list), s, action))\n",
    "        if sigma > 0:\n",
    "            sigma = np.sqrt(sigma)\n",
    "        else:\n",
    "            sigma = 0\n",
    "        Q.append(mu_t(prior, action, reward_list, w, alpha, thetas) + beta ** (-1)* sigma)\n",
    "    best_action = np.argmax(Q)\n",
    "    for action in actions:\n",
    "        if action == best_action:\n",
    "            action_vals.append(1 - epsilon)\n",
    "        else:\n",
    "            action_vals.append(epsilon / len(actions))\n",
    "    actions_normed = np.array(action_vals) / sum(action_vals)\n",
    "    return np.random.choice(len(actions), 1, p=actions_normed)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using UCB Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7, 9, 4],\n",
       "       [5, 5, 7],\n",
       "       [0, 5, 1],\n",
       "       [0, 5, 3],\n",
       "       [4, 6, 5],\n",
       "       [6, 4, 8],\n",
       "       [9, 4, 2],\n",
       "       [2, 9, 7],\n",
       "       [6, 3, 9],\n",
       "       [2, 4, 6]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "past_actions = []\n",
    "past_rewards = []\n",
    "exp_rewards = {}\n",
    "tau = 2\n",
    "lambda0 = 1\n",
    "w = 0.7\n",
    "alpha = 1.5\n",
    "beta = 5\n",
    "# total_actions = range(2)\n",
    "# rewards = [[10, 0], [0, 10], [5, 0], [0, 5],[10, 0], [0, 10], [5, 0], [0, 5],[10, 0], [0, 10], [5, 0], [0, 5]]\n",
    "total_actions = range(3)\n",
    "#rewards = [[10, 0, 0], [0, 10, 0], [0, 0, 10],[10, 0, 0], [0, 10, 0], [0, 0, 10], [10, 0, 0], [0, 10, 0],[10, 0, 0], [0, 10, 0], [0, 0, 10]]\n",
    "rewards = np.random.randint(10, size=(10, 3))\n",
    "states = [0]\n",
    "s_list = [0]\n",
    "epsilon = 1\n",
    "inc = 0.03\n",
    "post = [1]\n",
    "random_bot_rewards = []\n",
    "simple_bot_actions = np.zeros(len(total_actions))\n",
    "simple_bot_rewards = []\n",
    "for i in range(100):\n",
    "    curr_action = UCB_action(post, total_actions, past_actions, past_rewards, w, alpha, exp_rewards, beta, epsilon)\n",
    "    curr_reward = np.random.normal(rewards[states[-1]][curr_action], 1.0/tau)\n",
    "    #curr_reward = rewards[states[-1]][curr_action]\n",
    "    random_bot_rewards.append(np.random.normal(rewards[states[-1]][np.random.randint(len(total_actions))], 1.0/tau))\n",
    "    simple_bot_rewards.append(np.random.normal(rewards[states[-1]] \\\n",
    "                              [np.argmax(simple_bot_actions)], 1.0/tau))\n",
    "    simple_bot_actions[np.argmax(simple_bot_actions)] = simple_bot_rewards[-1]\n",
    "    #print(curr_action, curr_reward)\n",
    "    past_actions.append(curr_action)\n",
    "    past_rewards.append(curr_reward)\n",
    "    prior = prior_map_est(past_rewards[:-1], past_actions[:-1], w, alpha, exp_rewards)\n",
    "    #prior = prior_map_est(past_rewards, past_actions, w, alpha, exp_rewards)\n",
    "    post = posterior_norm(prior, past_actions[-1], past_rewards, w, alpha, exp_rewards)\n",
    "    #print(post)\n",
    "    exp_rewards = update_thetas(exp_rewards, curr_reward, curr_action, post, len(past_actions))\n",
    "    #print(exp_rewards)\n",
    "    #print()\n",
    "    state_probs = prior_norm(w, alpha, states)\n",
    "    #print('state_probs: ', state_probs)\n",
    "    states.append(np.random.choice(len(state_probs), p=state_probs))\n",
    "    if epsilon > inc:\n",
    "        epsilon -= inc\n",
    "# ax = sns.countplot(past_actions)\n",
    "# ax.set(xlabel='Arm selected', ylabel='Number of times arm was selected', title='Arm Selection with UCB Method')\n",
    "# for p in ax.patches:\n",
    "#     ax.annotate('r = ' + str(rewards[int(p.get_x()+0.4)]), (p.get_x() + 0.23, p.get_height() +0.25))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468.1745106770399"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The current agent\n",
    "sum(past_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376.81694282601603"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random actions\n",
    "sum(random_bot_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "383.32151108773235"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remembers the last reward for each arm, picks the arm that had the biggest last reward\n",
    "sum(simple_bot_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculated number of times each state has been visited**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 10, 1: 9, 2: 1, 3: 2, 4: 6, 5: 2, 6: 1})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 9.9967600269665322,\n",
       " (0, 1): 2.9415907287332725,\n",
       " (1, 0): 0.89142927104979397,\n",
       " (1, 1): 0.13193251578597834,\n",
       " (2, 0): 0.0049081323519856749,\n",
       " (2, 1): 0.43164144830557533,\n",
       " (3, 0): 1.9263951042410362e-41,\n",
       " (3, 1): 0.054011670533130676}"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0], array([ 0.90566038,  0.09433962])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_map_est([0, 0, 0],[0, 5, 5], [0, 1, 1], w, alpha, exp_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9285714285714286"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda0=0.5\n",
    "tau ** (-1) + lambda_t(2, 0, 1) ** (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
